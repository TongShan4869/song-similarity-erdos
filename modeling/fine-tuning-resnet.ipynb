{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine Tuning Resnet\n","- Prepare data for training/validation. Create dataloader\n","- Load in resnet model\n","- Create architecture for fine tuning including pytorch/tensorflow boilerplate"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#!pip install -q --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu118"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T14:13:25.602026Z","iopub.status.busy":"2024-08-15T14:13:25.601417Z","iopub.status.idle":"2024-08-15T14:13:40.258639Z","shell.execute_reply":"2024-08-15T14:13:40.257455Z","shell.execute_reply.started":"2024-08-15T14:13:25.601988Z"},"trusted":true},"outputs":[],"source":["!pip install -q spotipy"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-16T03:55:40.770776Z","iopub.status.busy":"2024-08-16T03:55:40.770175Z","iopub.status.idle":"2024-08-16T03:55:40.777397Z","shell.execute_reply":"2024-08-16T03:55:40.776408Z","shell.execute_reply.started":"2024-08-16T03:55:40.770744Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import librosa\n","import librosa.display\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","from dotenv import dotenv_values \n","#import spotipy\n","#from spotipy.oauth2 import SpotifyClientCredentials\n","import pickle as pkl\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optim\n","\n","from scipy.spatial.distance import euclidean\n","from sklearn.model_selection import train_test_split\n","from transformers import Wav2Vec2Model"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation\n","- Import data\n","- Calculate mel specs\n","- Create data set class for spectrograms/chromagrams/tempograms"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["explore_df = pd.read_pickle('/Users/reggiebain/erdos/song-similarity-erdos-old/data/augmented_audio/batch_1_augmented.pkl')\n","explore_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_mel_spectrogram(y, sr, n_mels=128, fmax=8000):\n","    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)\n","    S_dB = librosa.power_to_db(S, ref=np.max)\n","    return S_dB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply the function to the DataFrame\n","tqdm.pandas(desc=f\"Making Anchor Mel Spectrograms...\")\n","working_df['anchors'] = working_df['processed_audio'].progress_apply(lambda row: make_mel_spectrogram(row[0], row[1]))\n","tqdm.pandas(desc=f\"Making Similar Mel Spectrograms...\")\n","working_df['positives'] = working_df['augmented_audio'].progress_apply(lambda row: make_mel_spectrogram(row[0], row[1]))\n","tqdm.pandas(desc=f\"Making Differet Mel Spectrograms...\")\n","working_df['negatives'] = working_df['diff_processed_audio'].progress_apply(lambda row: make_mel_spectrogram(row[0], row[1]))\n","\n","# Create new dataframe\n","dataset_df = working_df[['anchors', 'positives', 'negatives']]\n","dataset_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot sample just to check\n","test_mel_spec = dataset_df['anchors'].iloc[0]\n","\n","plt.figure(figsize=(10, 4))\n","librosa.display.specshow(test_mel_spec, sr=22050, x_axis='time', y_axis='mel', fmax=8000)\n","plt.colorbar(format='%+2.0f dB')\n","plt.title('Mel Spectrogram')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Create Dataset Class"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-08-14T16:44:25.873612Z","iopub.status.busy":"2024-08-14T16:44:25.873240Z","iopub.status.idle":"2024-08-14T16:44:25.890245Z","shell.execute_reply":"2024-08-14T16:44:25.889288Z","shell.execute_reply.started":"2024-08-14T16:44:25.873584Z"},"trusted":true},"outputs":[],"source":["# Dataset class that does include batching\n","class SpectrogramDataset(Dataset):\n","\n","    def __init__(self, file_paths, sr=22050, n_mels=128):\n","        self.file_paths = file_paths\n","        self.current_file_index = 0\n","        self.current_file_length = 0\n","        self.current_data = None\n","        self.sr = sr\n","        self.n_mels = n_mels\n","        self.load_current_file()      \n","    \n","    def load_current_file(self):\n","        # Load data from the current .pkl file if we have any files left\n","        if self.current_file_index < len(self.file_paths):\n","            print(f\"Loading file {self.file_paths[self.current_file_index]}\")\n","            self.current_data = pd.read_pickle(self.file_paths[self.current_file_index])\n","            self.current_anchors = self.current_data['processed_audio'].values\n","            self.current_positives = self.current_data['augmented_audio'].values\n","            self.current_negatives = self.current_data['diff_processed_audio'].values\n","            self.current_file_length = len(self.current_anchors)\n","            print(f\"Loaded {self.current_file_length} samples from file {self.file_paths[self.current_file_index]}\")\n","            self.current_file_index += 1\n","        # If no remaining files, set to zero so we don't do anything\n","        else:\n","            self.current_data = None\n","            self.current_file_length = 0\n","            print('No more files to load.')\n","    \n","    def __len__(self):\n","        # Return the total length of all datasets in all pickle files\n","        total_length = sum(pd.read_pickle(file).shape[0] for file in self.file_paths)\n","        return total_length\n","    \n","    def __getitem__(self, idx):\n","        # Go through files until you find the index that you need to add more data\n","        #while self.current_data is not None and idx >= self.current_file_length:\n","        #    print(f\"Index {idx} exceeds current file length {self.current_file_length}. Loading next file.\")\n","        #    idx -= self.current_file_length\n","        #    self.load_current_file()\n","        #if self.current_data is None:\n","        #    raise IndexError('Index out of range as no more data is available.')\n","        print(f\"Starting __getitem__ with idx: {idx}\")\n","        while idx >= self.current_file_length:\n","            print(f\"Index {idx} exceeds current file length {self.current_file_length}. Loading next file.\")\n","            idx -= self.current_file_length\n","            if self.current_file_index < len(self.file_paths):\n","                self.load_current_file()\n","            else:\n","                raise IndexError(\"Index out of range inside while loop\")\n","        if self.current_data is None:\n","            raise IndexError(\"index out of range outside while loop\")\n","        print(f\"Accessing index {idx} in current file.\")\n","        # Load audio data and select idx'th example and get [0] to get audio from (y, sr) tuple\n","        anchor = self.current_anchors[idx][0]\n","        positive = self.current_positives[idx][0]\n","        negative = self.current_negatives[idx][0]\n","\n","        # Compute mel spectrograms\n","        anchor_mel = self._process_audio(anchor)\n","        positive_mel = self._process_audio(positive)\n","        negative_mel = self._process_audio(negative)\n","\n","        return anchor_mel, positive_mel, negative_mel\n","    \n","    # Convert raw audio to mel spectrogram\n","    def _process_audio(self, y):\n","        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n","        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","        mel_spectrogram_db = torch.tensor(mel_spectrogram_db, dtype=torch.float32).unsqueeze(0)\n","        return mel_spectrogram_db"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-16T03:55:46.409529Z","iopub.status.busy":"2024-08-16T03:55:46.409113Z","iopub.status.idle":"2024-08-16T03:55:46.426042Z","shell.execute_reply":"2024-08-16T03:55:46.425065Z","shell.execute_reply.started":"2024-08-16T03:55:46.409495Z"},"trusted":true},"outputs":[],"source":["class SpectrogramDataset(Dataset):\n","    def __init__(self, file_paths, transform=False, sr=22050, n_mels=128):\n","        self.file_paths = file_paths\n","        self.data_index = self._build_index()\n","        self.sr = sr\n","        self.n_mels = n_mels\n","        self.transform = transform\n","        \n","    def _build_index(self):\n","        index = []\n","        for file_idx, file_path in enumerate(self.file_paths):\n","            with open(file_path, 'rb') as f:\n","                data = pkl.load(f)\n","                for i in range(len(data)):\n","                    index.append((file_idx, i))\n","        return index\n","\n","    def _get_log_mel_spectrogram(self, y):\n","        # Convert to mel spectrogram\n","        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n","        # Convert to log scale\n","        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","        log_mel_spectrogram = torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)\n","        return log_mel_spectrogram\n","\n","    def __len__(self):\n","        return len(self.data_index)\n","\n","    def __getitem__(self, idx):\n","        file_idx, data_idx = self.data_index[idx]\n","        file_path = self.file_paths[file_idx]\n","\n","        with open(file_path, 'rb') as f:\n","            data = pkl.load(f)\n","        \n","        row = data.iloc[data_idx]\n","        anchor = row['processed_audio'][0]  # (y, sr)\n","        positive = row['augmented_audio'][0]\n","        negative = row['diff_processed_audio'][0]\n","\n","        # Convert to log mel spectrograms\n","        anchor_mel = self._get_log_mel_spectrogram(anchor)\n","        positive_mel = self._get_log_mel_spectrogram(positive)\n","        negative_mel = self._get_log_mel_spectrogram(negative)\n","        \n","        # Apply any transformations\n","        if self.transform:\n","            anchors = self.transform(anchors)\n","            positives = self.transform(positives)\n","            negatives = self.transform(negatives)\n","\n","        return anchor_mel, positive_mel, negative_mel\n"]},{"cell_type":"markdown","metadata":{},"source":["### Split Data and Instantiate Dataset Class"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-16T03:55:50.830474Z","iopub.status.busy":"2024-08-16T03:55:50.830123Z","iopub.status.idle":"2024-08-16T03:58:11.297085Z","shell.execute_reply":"2024-08-16T03:58:11.296087Z","shell.execute_reply.started":"2024-08-16T03:55:50.830446Z"},"trusted":true},"outputs":[],"source":["file_paths = [f'/Users/reggiebain/erdos/song-similarity-erdos-old/data/augmented_audio/batch_{i}_augmented.pkl' for i in range(1,10,1)]\n","\n","# Split the files instaed of actual data into training/val\n","train_files, val_files = train_test_split(file_paths, test_size=0.2, random_state=123)\n","\n","# Instantiate Dataset Classes\n","train_dataset = SpectrogramDataset(train_files)\n","val_dataset = SpectrogramDataset(val_files)\n","\n","# Declare dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,  pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False,pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Delcaring the Model\n","- Define architecture: default resnet with adjusted first conv layer and final FC layer to set num params\n","- Choose loss function, optimizer, device, etc."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-16T03:58:11.298977Z","iopub.status.busy":"2024-08-16T03:58:11.298624Z","iopub.status.idle":"2024-08-16T03:58:11.306149Z","shell.execute_reply":"2024-08-16T03:58:11.305198Z","shell.execute_reply.started":"2024-08-16T03:58:11.298950Z"},"trusted":true},"outputs":[],"source":["# Define pretrained resnet from Torch Vision resnet 18\n","class ResNetEmbedding(nn.Module):\n","    def __init__(self, embedding_dim=128, dropout_rate=0.5):\n","        # get resnet super class\n","        super(ResNetEmbedding, self).__init__()\n","        self.resnet = models.resnet18(weights='DEFAULT')\n","        # Change structure of first layer to take non RGB images, rest of params same as default\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        #self.dropout = nn.Dropout(p=dropout_rate)\n","        # Set the last fully connected to a set dimension \"embedding_dim\" instead of default 1000\n","        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embedding_dim)\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        return F.normalize(x, p=2, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import WhisperProcessor, WhisperForConditionalGeneration\n","import torch\n","import torch.nn as nn\n","\n","class WhisperEmbedding(nn.Module):\n","    def __init__(self, embedding_dim=128, pretrained_model_name=\"openai/whisper-tiny\"):\n","        super(WhisperEmbedding, self).__init__()\n","        self.whisper = WhisperForConditionalGeneration.from_pretrained(pretrained_model_name)\n","        self.fc = nn.Linear(self.whisper.config.hidden_size, embedding_dim)\n","\n","    def forward(self, x):\n","        # Pass through Whisper's encoder\n","        outputs = self.whisper.encoder(x)\n","        # Get the mean of the hidden states to create an embedding\n","        embedding = outputs.last_hidden_state.mean(dim=1)\n","        # Pass through a linear layer to get the final embedding\n","        embedding = self.fc(embedding)\n","        # Normalize the embedding\n","        return torch.nn.functional.normalize(embedding, p=2, dim=1)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-16T03:58:12.880987Z","iopub.status.busy":"2024-08-16T03:58:12.880706Z","iopub.status.idle":"2024-08-16T03:58:13.131753Z","shell.execute_reply":"2024-08-16T03:58:13.130801Z","shell.execute_reply.started":"2024-08-16T03:58:12.880963Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ResNetEmbedding(\n","  (resnet): ResNet(\n","    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=128, bias=True)\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Choose model, loss, and optimizer\n","model = ResNetEmbedding()\n","criterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n","\n","# Freeze all layers except the last block and the fully connected layer\n","for param in model.resnet.parameters():\n","    param.requires_grad = False\n","\n","for param in model.resnet.layer4.parameters():\n","    param.requires_grad = True\n","\n","for param in model.resnet.fc.parameters():\n","    param.requires_grad = True\n","\n","# Use a smaller learning rate for fine-tuning\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n","#optimizer = optim.Adam(lr=1e-5, weight_decay=1e-4)\n","\n","# Declare losses/accuracies\n","train_losses = []\n","val_losses = []\n","baseline_losses = []\n","\n","num_epochs = 5\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Training Model\n","- Go in training mode, zero gradients with "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-16T04:08:26.714336Z","iopub.status.busy":"2024-08-16T04:08:26.713971Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training 1/5:   0%|          | 0/110 [02:02<?, ?batch/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m anchors, positives, negatives \u001b[38;5;241m=\u001b[39m anchors\u001b[38;5;241m.\u001b[39mto(device), positives\u001b[38;5;241m.\u001b[39mto(device), negatives\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m anchor_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m positive_embeddings \u001b[38;5;241m=\u001b[39m model(positives)\n\u001b[1;32m     15\u001b[0m negative_embeddings \u001b[38;5;241m=\u001b[39m model(negatives)\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mResNetEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torchvision/models/resnet.py:271\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m--> 271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/_jit_internal.py:497\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/erdos_may_2024/lib/python3.11/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Loop over epochs\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    running_train_loss = 0.0\n","    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\", unit=\"batch\")\n","\n","    # Loop over batches using dataloaders\n","    for anchors, positives, negatives in train_loader:\n","        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n","        optimizer.zero_grad()\n","        \n","        anchor_embeddings = model(anchors)\n","        positive_embeddings = model(positives)\n","        negative_embeddings = model(negatives)\n","        \n","        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_train_loss += loss.item() * anchors.size(0)\n","        # Update the progress bar by the current batch size\n","        pbar.update(1)  # Increment the progress bar\n","        #pbar.update(anchors.size(0))\n","        #pbar.set_postfix(loss=loss.item())\n","    \n","          \n","    train_loss = running_train_loss / len(train_loader.dataset)\n","    train_losses.append(train_loss)\n","        #train_loss += loss.item()\n","\n","    # Get avg train loss over batches\n","    #train_loss /= len(train_loader)\n","    \n","    # Turn on validation/eval mode\n","    model.eval()\n","    running_val_loss = 0.0 \n","    baseline_loss = 0.0   \n","    val_pbar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\")\n","    # Turn off gradient updates since we're in validation\n","    with torch.no_grad():\n","        # Batch loop \n","        for anchors, positives, negatives in val_loader:\n","            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n","            \n","            anchor_embeddings = model(anchors)\n","            positive_embeddings = model(positives)\n","            negative_embeddings = model(negatives)\n","            \n","            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n","            #val_loss += loss.item()\n","\n","            running_val_loss += loss.item() * anchors.size(0)\n","            # baseline loss\n","            baseline_loss += criterion(anchors, positives, negatives).item()\n","            \n","            # Update the validation progress bar\n","            val_pbar.update(1)  # Increment the progress bar\n","            #val_pbar.update(anchors.size(0))\n","            #val_pbar.set_postfix(loss=loss.item())\n","    \n","    # Calculate average validation loss over the entire dataset\n","    val_loss = running_val_loss / len(val_loader.dataset)\n","    val_losses.append(val_loss)  \n","\n","    baseline_avg_loss = baseline_loss / len(val_loader.dataset)\n","    baseline_losses.append(baseline_avg_loss)\n","    # Get average val loss over batches\n","    #val_loss /= len(val_loader)\n","    \n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Baseline Loss: {baseline_avg_loss:.4f}\")\n","    \n","    with open('training_logs.pkl', 'wb') as f:\n","        pkl.dump((train_losses, val_losses), f)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot loss curves for training\n","epochs = range(1, num_epochs + 1)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(epochs, train_losses, label='Training Loss')\n","plt.plot(epochs, val_losses, label='Validation Loss')\n","plt.plot(epochs, baseline_losses, label='Baseline Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig('resnet-loss-plot.png')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T16:29:15.974133Z","iopub.status.busy":"2024-08-15T16:29:15.973816Z","iopub.status.idle":"2024-08-15T16:29:16.084584Z","shell.execute_reply":"2024-08-15T16:29:16.083586Z","shell.execute_reply.started":"2024-08-15T16:29:15.974108Z"},"trusted":true},"outputs":[],"source":["# Save just the model weights (recommended apparently for portability/compatibility)\n","torch.save(model.state_dict(), 'resnet18_model_weights.pth')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-15T16:29:21.286557Z","iopub.status.busy":"2024-08-15T16:29:21.286231Z","iopub.status.idle":"2024-08-15T16:29:21.372398Z","shell.execute_reply":"2024-08-15T16:29:21.371515Z","shell.execute_reply.started":"2024-08-15T16:29:21.286532Z"},"trusted":true},"outputs":[],"source":["# Save the entire model so we can use it for deployment\n","torch.save(model, 'resnet18_model.pth')"]},{"cell_type":"markdown","metadata":{},"source":["### Deploy on any 2 songs\n","- Use model to calculate embeddings (using eval mode specifically and with no gradient updating)\n","- NOTE: Right now, we have to have the input data in the correct format: a spectrogram/chromagram/tempogram (generically called \"gram\"). So for any deployment, we'll have to do preprocessing in the streamlit app for example. OR we can have a set of say 10-15 sample songs you can compare where we've already done all of the calculations.\n","- **Similarity values key:**\n","    - 0.5 to 1: Very similar. Perhaps the same song.\n","    - 0 to 0.5: Somewhat similar. Share some key characteristics\n","    - -1 to 0: Low to no similarity. Different songs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# How to load the model later using just the state dictionary\n","model = ResNetEmbedding()  # Make sure this matches the architecture you used\n","model.load_state_dict(torch.load('resnet18_model_weights.pth'))\n","\n","# If using a GPU\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def extract_embedding(model, audio_data_clip, sr=22050, use_model=True):\n","    y = audio_data_clip\n","    #y, sr = librosa.load(audio_data_clip, sr=sr)\n","    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","    \n","    # Convert to tensor and move to the appropriate device\n","    mel_tensor = torch.tensor(mel_spectrogram_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n","    \n","    if use_model:\n","        # Get the embedding from the model\n","        with torch.no_grad():\n","            embedding = model(mel_tensor)\n","        \n","        # Normalize the embedding\n","        #embedding = F.normalize(embedding, p=2, dim=1)\n","        return embedding\n","    else:\n","        return mel_tensor\n","\n","\n","def compute_cosine_similarity(embedding1, embedding2):\n","    # Compute cosine similarity\n","    cosine_sim = F.cosine_similarity(embedding1, embedding2)\n","    return cosine_sim.item()  # Convert to a Python float\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["explore_df = pd.read_pickle('/kaggle/input/augmented-audio-10k/batch_1_augmented.pkl')\n","explore_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test on some random training/validation data as sanity check\n","y1, y2 = explore_df['processed_audio'][10000][0], explore_df['diff_processed_audio'][10000][0]\n","mel1, mel2 = extract_embedding(model, y1), extract_embedding(model, y2)\n","compute_cosine_similarity(mel1, mel2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Test on some random training/validation data as sanity check\n","y1, y3 = explore_df['processed_audio'][10000][0], explore_df['augmented_audio'][10000][0]\n","mel1, mel3 = extract_embedding(model, y1), extract_embedding(model, y3)\n","compute_cosine_similarity(mel1, mel3)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion(mel1, mel2, mel3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Paths to the audio files (these should be the paths to your actual song files)\n","anchor = '/Users/reggiebain/erdos/song-similarity-erdos-old/data/coversongs/covers32k/A_Whiter_Shade_Of_Pale/annie_lennox+Medusa+03-A_Whiter_Shade_Of_Pale.mp3'\n","positive = '/Users/reggiebain/erdos/song-similarity-erdos-old/data/coversongs/covers32k/A_Whiter_Shade_Of_Pale/procol_harum+Greatest_Hits+2-A_Whiter_Shade_Of_Pale.mp3'\n","negative = '/Users/reggiebain/erdos/song-similarity-erdos-old/data/coversongs/covers32k/Abracadabra/steve_miller_band+Steve_Miller_Band_Live_+09-Abracadabra.mp3'\n","\n","# Extract embeddings for both songs\n","embedding1 = extract_embedding(model, anchor)\n","embedding2 = extract_embedding(model, positive)\n","embedding3 = extract_embedding(model, negative)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate cosine similarity between the two embeddings\n","print(f\"Cosine Similarity between song and known cover: {compute_cosine_similarity(embedding1, embedding2):.4f}\")\n","print(f\"Cosine Similarity between song and random other song: {compute_cosine_similarity(embedding1, embedding3):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["compute_cosine_similarity(embedding2, embedding3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embed1 = extract_embedding(model, explore_df.processed_audio[10000][0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Cosine Similarity between song and known cover (raw): {compute_cosine_similarity(embedding1, embedding2, use_model=False):.4f}\")\n","print(f\"Cosine Similarity between song and random other song (raw): {compute_cosine_similarity(embedding1, embedding3, use_model=False):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Try triplet loss between these 3 songs\n","print(f\"Triplet Loss between songs: {criterion(embedding1, embedding2, embedding3)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# try using baseline\n","baseline_anchor = extract_embedding(model, anchor, use_model=False)\n","baseline_positive = extract_embedding(model, positive, use_model=False)\n","baseline_negative = extract_embedding(model, negative, use_model=False)\n","#print(f\"Triplet loss with baseline: {criterion(baseline_anchor, baseline_positive, baseline_negative)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["baseline_anchor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["baseline_positive.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["F.normalize(baseline_positive, p=2, dim=1).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5535463,"sourceId":9162188,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
