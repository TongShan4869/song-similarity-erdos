{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9162188,"sourceType":"datasetVersion","datasetId":5535463}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Resnet\n- Prepare data for training/validation. Create dataloader\n- Load in resnet model\n- Create architecture for fine tuning including pytorch/tensorflow boilerplate","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nfrom dotenv import dotenv_values \nimport pickle as pkl\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\n\nfrom scipy.spatial.distance import euclidean\nfrom sklearn.model_selection import train_test_split\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration","metadata":{"execution":{"iopub.status.busy":"2024-08-20T03:58:18.972348Z","iopub.execute_input":"2024-08-20T03:58:18.972797Z","iopub.status.idle":"2024-08-20T03:58:27.922843Z","shell.execute_reply.started":"2024-08-20T03:58:18.972760Z","shell.execute_reply":"2024-08-20T03:58:27.921149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create Dataset Class","metadata":{}},{"cell_type":"code","source":"class SpectrogramDataset(Dataset):\n    def __init__(self, file_paths, transform=False, sr=22050, n_mels=128):\n        self.file_paths = file_paths\n        self.data_index = self._build_index()\n        self.sr = sr\n        self.n_mels = n_mels\n        self.transform = transform\n        \n    def _build_index(self):\n        index = []\n        for file_idx, file_path in enumerate(self.file_paths):\n            with open(file_path, 'rb') as f:\n                data = pkl.load(f)\n                for i in range(len(data)):\n                    index.append((file_idx, i))\n        return index\n\n    def _get_log_mel_spectrogram(self, y):\n        # Convert to mel spectrogram\n        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n        # Convert to log scale\n        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n        log_mel_spectrogram = torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)\n        return log_mel_spectrogram\n\n    def __len__(self):\n        return len(self.data_index)\n\n    def __getitem__(self, idx):\n        file_idx, data_idx = self.data_index[idx]\n        file_path = self.file_paths[file_idx]\n\n        with open(file_path, 'rb') as f:\n            data = pkl.load(f)\n        \n        row = data.iloc[data_idx]\n        anchor = row['processed_audio'][0]  # (y, sr)\n        positive = row['augmented_audio'][0]\n        negative = row['diff_processed_audio'][0]\n\n        # Convert to log mel spectrograms\n        anchor_mel = self._get_log_mel_spectrogram(anchor)\n        positive_mel = self._get_log_mel_spectrogram(positive)\n        negative_mel = self._get_log_mel_spectrogram(negative)\n        \n        # Apply any transformations\n        if self.transform:\n            anchors = self.transform(anchors)\n            positives = self.transform(positives)\n            negatives = self.transform(negatives)\n\n        return anchor_mel, positive_mel, negative_mel\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T23:07:42.161610Z","iopub.execute_input":"2024-08-19T23:07:42.162162Z","iopub.status.idle":"2024-08-19T23:07:42.175092Z","shell.execute_reply.started":"2024-08-19T23:07:42.162129Z","shell.execute_reply":"2024-08-19T23:07:42.173872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split Data and Instantiate Dataset Class","metadata":{}},{"cell_type":"code","source":"file_paths = [f'/kaggle/input/augmented-audio-10k/batch_{i}_augmented.pkl' for i in range(1,10,1)]\n\n# Split the files instaed of actual data into training/val\ntrain_files, val_files = train_test_split(file_paths, test_size=0.2, random_state=123)\n\n# Instantiate Dataset Classes\ntrain_dataset = SpectrogramDataset(train_files)\nval_dataset = SpectrogramDataset(val_files)\n\n# Declare dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)#, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T23:07:42.176536Z","iopub.execute_input":"2024-08-19T23:07:42.176883Z","iopub.status.idle":"2024-08-19T23:10:34.029520Z","shell.execute_reply.started":"2024-08-19T23:07:42.176852Z","shell.execute_reply":"2024-08-19T23:10:34.026577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(val_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T23:25:06.674881Z","iopub.execute_input":"2024-08-19T23:25:06.675367Z","iopub.status.idle":"2024-08-19T23:25:06.683446Z","shell.execute_reply.started":"2024-08-19T23:25:06.675333Z","shell.execute_reply":"2024-08-19T23:25:06.682000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Delcaring the Model\n- Define architecture: default resnet with adjusted first conv layer and final FC layer to set num params\n- Choose loss function, optimizer, device, etc.","metadata":{}},{"cell_type":"code","source":"# Define pretrained resnet from Torch Vision resnet 18\nclass ResNetEmbedding(nn.Module):\n    def __init__(self, embedding_dim=128, dropout_rate=0.5):\n        # get resnet super class\n        super(ResNetEmbedding, self).__init__()\n        self.resnet = models.resnet18(weights='DEFAULT')\n        # Change structure of first layer to take non RGB images, rest of params same as default\n        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.dropout = nn.Dropout(p=dropout_rate)\n        # Set the last fully connected to a set dimension \"embedding_dim\" instead of default 1000\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embedding_dim)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        return F.normalize(x, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T23:17:35.660307Z","iopub.execute_input":"2024-08-19T23:17:35.662549Z","iopub.status.idle":"2024-08-19T23:17:35.676843Z","shell.execute_reply.started":"2024-08-19T23:17:35.662501Z","shell.execute_reply":"2024-08-19T23:17:35.675637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MusicSimilarityCNN(nn.Module):\n    def __init__(self, embedding_dim=128, dropout_rate=0.5):\n        super(MusicSimilarityCNN, self).__init__()\n        \n        # Layers to get to 128 dim embeddings\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        \n        # Batch norm for each \n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Pooling\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        \n        # FCs\n        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n        self.fc2 = nn.Linear(256, embedding_dim)\n        \n    def forward(self, x):\n        # CNN layers with ReLU activation and pooling\n        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        \n        # Flatten the output for the fully connected layer\n        x = x.view(-1, 128 * 8 * 8)  # Adjust this according to the input size\n        x = F.relu(self.fc1(x))\n        \n        # Output the embeddings\n        x = self.fc2(x)\n        return F.normalize(x, p=2, dim=1)  # Normalize embeddings for better similarity comparison\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T03:58:27.925139Z","iopub.execute_input":"2024-08-20T03:58:27.925782Z","iopub.status.idle":"2024-08-20T03:58:27.940319Z","shell.execute_reply.started":"2024-08-20T03:58:27.925744Z","shell.execute_reply":"2024-08-20T03:58:27.938876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose model, loss, and optimizer\nmodel = ResNetEmbedding()\n#model = MusicSimilarityCNN()\n\ncriterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n\n# Freeze all the layers\nfor param in model.resnet.parameters():\n    param.requires_grad = False\n\n# Turn back on last residual block\nfor param in model.resnet.layer4.parameters():\n    param.requires_grad = True\n\n# Turn back on fully connected layer\nfor param in model.resnet.fc.parameters():\n    param.requires_grad = True\n\n# Use a smaller learning rate for fine-tuning\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n\n#optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n\n# Declare losses/accuracies\ntrain_losses = []\nval_losses = []\nbaseline_losses = []\n\nnum_epochs = 5\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T22:03:06.734192Z","iopub.execute_input":"2024-08-18T22:03:06.734510Z","iopub.status.idle":"2024-08-18T22:03:07.699445Z","shell.execute_reply.started":"2024-08-18T22:03:06.734480Z","shell.execute_reply":"2024-08-18T22:03:07.698532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Model","metadata":{}},{"cell_type":"code","source":"# Loop over epochs\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    running_train_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\", unit=\"batch\")\n\n    # Loop over batches using dataloaders\n    for anchors, positives, negatives in train_loader:\n        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n        optimizer.zero_grad()\n        \n        anchor_embeddings = model(anchors)\n        positive_embeddings = model(positives)\n        negative_embeddings = model(negatives)\n        \n        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n        loss.backward()\n        optimizer.step()\n        \n        running_train_loss += loss.item() * anchors.size(0)\n        # Update the progress bar by the current batch size\n        pbar.update(1)  # Increment the progress bar\n        #pbar.update(anchors.size(0))\n        #pbar.set_postfix(loss=loss.item())\n          \n    train_loss = running_train_loss / len(train_loader.dataset)\n    train_losses.append(train_loss)\n\n    # Turn on validation/eval mode\n    model.eval()\n    running_val_loss = 0.0 \n    running_baseline_loss = 0.0   \n    val_pbar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\")\n    # Turn off gradient updates since we're in validation\n    with torch.no_grad():\n        # Batch loop \n        for anchors, positives, negatives in tqdm(val_loader):\n            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n            \n            anchor_embeddings = model(anchors)\n            positive_embeddings = model(positives)\n            negative_embeddings = model(negatives)\n            \n            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n\n            # Add to running val loss\n            running_val_loss += loss.item() * anchors.size(0)\n            \n            # baseline loss\n            baseline_loss = criterion(F.normalize(anchors), \n                                               F.normalize(positives), \n                                               F.normalize(negatives)).item()\n            running_baseline_loss += baseline_loss*anchors.size(0)\n            \n            # Update the validation progress bar\n            val_pbar.update(1)  # Increment the progress bar\n            #val_pbar.update(anchors.size(0))\n            #val_pbar.set_postfix(loss=loss.item())\n    \n    # Calculate average validation loss over the entire dataset\n    val_loss = running_val_loss / len(val_loader.dataset)\n    val_losses.append(val_loss)\n    # Do the same for the baseline\n    baseline_avg_loss = running_baseline_loss / len(val_loader.dataset)\n    baseline_losses.append(baseline_avg_loss)\n\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Baseline Loss: {baseline_avg_loss:.4f}\")\n    \n    with open('training_logs.pkl', 'wb') as f:\n        pkl.dump((train_losses, val_losses, baseline_losses), f)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T22:03:07.701882Z","iopub.execute_input":"2024-08-18T22:03:07.702434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot loss curves for training\nepochs = range(1, num_epochs + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.plot(epochs, baseline_losses, label='Baseline Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.tight_layout()\nplt.savefig('resnet-loss-plot.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.439153Z","iopub.status.idle":"2024-08-16T16:22:41.439602Z","shell.execute_reply.started":"2024-08-16T16:22:41.439403Z","shell.execute_reply":"2024-08-16T16:22:41.439427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save just the model weights (recommended apparently for portability/compatibility)\ntorch.save(model.state_dict(), 'resnet18_model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.441097Z","iopub.status.idle":"2024-08-16T16:22:41.441510Z","shell.execute_reply.started":"2024-08-16T16:22:41.441304Z","shell.execute_reply":"2024-08-16T16:22:41.441321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the entire model so we can use it for deployment\ntorch.save(model, 'resnet18_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.443645Z","iopub.status.idle":"2024-08-16T16:22:41.444030Z","shell.execute_reply.started":"2024-08-16T16:22:41.443852Z","shell.execute_reply":"2024-08-16T16:22:41.443869Z"},"trusted":true},"execution_count":null,"outputs":[]}]}