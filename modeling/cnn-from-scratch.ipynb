{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9162188,"sourceType":"datasetVersion","datasetId":5535463}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CNN for Audio Analysis\n- Create basic CNN from scratch to make 128 dimensional embeddings\n- Train on the triplet loss to\n    1. Minimize euclidean distance between anchor positive\n    2. Maximize euclidean distance between anchor negative\n- Output model weights for deployment on songs with real covers     ","metadata":{}},{"cell_type":"markdown","source":"#### Import Dependencies","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nfrom dotenv import dotenv_values \nimport pickle as pkl\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\n\nfrom scipy.spatial.distance import euclidean\nfrom sklearn.model_selection import train_test_split\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration","metadata":{"execution":{"iopub.status.busy":"2024-08-23T03:54:01.167156Z","iopub.execute_input":"2024-08-23T03:54:01.168020Z","iopub.status.idle":"2024-08-23T03:54:07.275734Z","shell.execute_reply.started":"2024-08-23T03:54:01.167975Z","shell.execute_reply":"2024-08-23T03:54:07.274844Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Create Dataset Class","metadata":{}},{"cell_type":"code","source":"class SpectrogramDataset(Dataset):\n    def __init__(self, file_paths, transform=False, sr=22050, n_mels=128):\n        self.file_paths = file_paths\n        self.data_index = self._build_index()\n        self.sr = sr\n        self.n_mels = n_mels\n        self.transform = transform\n        \n    def _build_index(self):\n        index = []\n        for file_idx, file_path in enumerate(self.file_paths):\n            with open(file_path, 'rb') as f:\n                data = pkl.load(f)\n                for i in range(len(data)):\n                    index.append((file_idx, i))\n        return index\n\n    def _get_log_mel_spectrogram(self, y):\n        # Convert to mel spectrogram\n        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n        # Convert to log scale\n        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n        log_mel_spectrogram = torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)\n        return log_mel_spectrogram\n\n    def __len__(self):\n        return len(self.data_index)\n\n    def __getitem__(self, idx):\n        file_idx, data_idx = self.data_index[idx]\n        file_path = self.file_paths[file_idx]\n\n        with open(file_path, 'rb') as f:\n            data = pkl.load(f)\n        \n        row = data.iloc[data_idx]\n        anchor = row['processed_audio'][0]  # (y, sr)\n        positive = row['augmented_audio'][0]\n        negative = row['diff_processed_audio'][0]\n\n        # Convert to log mel spectrograms\n        anchor_mel = self._get_log_mel_spectrogram(anchor)\n        positive_mel = self._get_log_mel_spectrogram(positive)\n        negative_mel = self._get_log_mel_spectrogram(negative)\n        \n        # Apply any transformations\n        if self.transform:\n            anchors = self.transform(anchors)\n            positives = self.transform(positives)\n            negatives = self.transform(negatives)\n\n        return anchor_mel, positive_mel, negative_mel\n","metadata":{"execution":{"iopub.status.busy":"2024-08-23T03:54:07.277297Z","iopub.execute_input":"2024-08-23T03:54:07.277775Z","iopub.status.idle":"2024-08-23T03:54:07.450191Z","shell.execute_reply.started":"2024-08-23T03:54:07.277747Z","shell.execute_reply":"2024-08-23T03:54:07.449125Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Split Data and Instantiate Dataset Class and DataLoaders","metadata":{}},{"cell_type":"code","source":"file_paths = [f'/kaggle/input/augmented-audio-10k/batch_{i}_augmented.pkl' for i in range(1,10,1)]\n\n# Split the files instaed of actual data into training/val\ntrain_files, val_files = train_test_split(file_paths, test_size=0.2, random_state=123)\n\n# Instantiate Dataset Classes\ntrain_dataset = SpectrogramDataset(train_files)\nval_dataset = SpectrogramDataset(val_files)\n\n# Declare dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)#, drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T03:54:07.451414Z","iopub.execute_input":"2024-08-23T03:54:07.451771Z","iopub.status.idle":"2024-08-23T03:57:05.165528Z","shell.execute_reply.started":"2024-08-23T03:54:07.451739Z","shell.execute_reply":"2024-08-23T03:57:05.164623Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_loader.dataset[0][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-23T03:57:05.167392Z","iopub.execute_input":"2024-08-23T03:57:05.167732Z","iopub.status.idle":"2024-08-23T03:57:22.023453Z","shell.execute_reply.started":"2024-08-23T03:57:05.167706Z","shell.execute_reply":"2024-08-23T03:57:22.022268Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"tensor([[[-14.7507, -13.5368, -15.8320,  ..., -15.6749, -16.3634, -16.5979],\n         [-16.1341, -12.1904, -14.2771,  ..., -17.0390, -16.9947, -16.8911],\n         [-14.2619, -12.5380, -13.1737,  ..., -18.4182, -18.0972, -18.9446],\n         ...,\n         [-53.5397, -49.8351, -51.9922,  ..., -55.5794, -56.0601, -48.8555],\n         [-60.9177, -56.7162, -61.3718,  ..., -58.9527, -58.1980, -51.9949],\n         [-66.4927, -64.5872, -70.4376,  ..., -80.0000, -76.9031, -69.2988]]])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Delcaring the Model\n- Define architecture: standard CNN with batch norms and pooling to create 128 dim embeddings\n- Choose loss function, optimizer, device, etc.","metadata":{}},{"cell_type":"code","source":"class MusicSimilarityCNN(nn.Module):\n    def __init__(self, embedding_dim=128, dropout_rate=0.5):\n        super(MusicSimilarityCNN, self).__init__()\n        \n        # Layers to get to 128 dim embeddings\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) \n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) \n        \n        # Batch norm for each \n        self.bn1 = nn.BatchNorm2d(32)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Pooling\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        \n        # FCs\n        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n        self.fc2 = nn.Linear(256, embedding_dim)\n        \n    def forward(self, x):\n        # CNN layers with ReLU activation and pooling - input [32, 1, 128,128]\n        x = self.pool(F.relu(self.bn1(self.conv1(x)))) # Conv -> BatchNorm -> Relu -> Pool\n        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n        \n        # Flatten the output for the fully connected layer\n        x = x.view(-1, 128 * 8 * 8) # 128*8*8 = 8192 --> Shape = []\n        x = F.relu(self.fc1(x))\n        \n        # Output the embeddings\n        x = self.fc2(x)\n        return F.normalize(x, p=2, dim=1)  # Normalize embeddings for better similarity comparison\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T03:58:27.925139Z","iopub.execute_input":"2024-08-20T03:58:27.925782Z","iopub.status.idle":"2024-08-20T03:58:27.940319Z","shell.execute_reply.started":"2024-08-20T03:58:27.925744Z","shell.execute_reply":"2024-08-20T03:58:27.938876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose model, loss, and optimizer\nmodel = MusicSimilarityCNN()\ncriterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\noptimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n\n# Declare losses/accuracies\ntrain_losses = []\nval_losses = []\nbaseline_losses = []\n\nnum_epochs = 16\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T22:03:06.734192Z","iopub.execute_input":"2024-08-18T22:03:06.734510Z","iopub.status.idle":"2024-08-18T22:03:07.699445Z","shell.execute_reply.started":"2024-08-18T22:03:06.734480Z","shell.execute_reply":"2024-08-18T22:03:07.698532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Model","metadata":{}},{"cell_type":"code","source":"# Loop over epochs\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    running_train_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\", unit=\"batch\")\n\n    # Loop over batches using dataloaders\n    for anchors, positives, negatives in train_loader:\n        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n        optimizer.zero_grad()\n        \n        anchor_embeddings = model(anchors)\n        positive_embeddings = model(positives)\n        negative_embeddings = model(negatives)\n        \n        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n        loss.backward()\n        optimizer.step()\n        \n        running_train_loss += loss.item() * anchors.size(0)\n        pbar.update(1)\n          \n    train_loss = running_train_loss / len(train_loader.dataset)\n    train_losses.append(train_loss)\n\n    # Turn on validation/eval mode\n    model.eval()\n    running_val_loss = 0.0 \n    running_baseline_loss = 0.0   \n    val_pbar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\")\n    \n    # Turn off gradient updates since we're in validation\n    with torch.no_grad():\n        # Batch loop \n        for anchors, positives, negatives in tqdm(val_loader):\n            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n            \n            anchor_embeddings = model(anchors)\n            positive_embeddings = model(positives)\n            negative_embeddings = model(negatives)\n            \n            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n\n            # Add to running val loss\n            running_val_loss += loss.item() * anchors.size(0)\n            \n            # baseline loss\n            baseline_loss = criterion(F.normalize(anchors), \n                                               F.normalize(positives), \n                                               F.normalize(negatives)).item()\n            running_baseline_loss += baseline_loss*anchors.size(0)\n            \n            # Update the validation progress bar\n            val_pbar.update(1)\n\n    \n    # Calculate average validation loss over the entire dataset\n    val_loss = running_val_loss / len(val_loader.dataset)\n    val_losses.append(val_loss)\n    # Do the same for the baseline\n    baseline_avg_loss = running_baseline_loss / len(val_loader.dataset)\n    baseline_losses.append(baseline_avg_loss)\n\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Baseline Loss: {baseline_avg_loss:.4f}\")\n    \n    with open('training_logs.pkl', 'wb') as f:\n        pkl.dump((train_losses, val_losses), f)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T22:03:07.701882Z","iopub.execute_input":"2024-08-18T22:03:07.702434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot loss curves for training\nepochs = range(1, num_epochs + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.plot(epochs, baseline_losses, label='Baseline Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.tight_layout()\nplt.savefig('cnn-loss-plot.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.439153Z","iopub.status.idle":"2024-08-16T16:22:41.439602Z","shell.execute_reply.started":"2024-08-16T16:22:41.439403Z","shell.execute_reply":"2024-08-16T16:22:41.439427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save just the model weights (recommended apparently for portability/compatibility)\ntorch.save(model.state_dict(), 'cnn_model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.441097Z","iopub.status.idle":"2024-08-16T16:22:41.441510Z","shell.execute_reply.started":"2024-08-16T16:22:41.441304Z","shell.execute_reply":"2024-08-16T16:22:41.441321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the entire model so we can use it for deployment\ntorch.save(model, 'cnn_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.443645Z","iopub.status.idle":"2024-08-16T16:22:41.444030Z","shell.execute_reply.started":"2024-08-16T16:22:41.443852Z","shell.execute_reply":"2024-08-16T16:22:41.443869Z"},"trusted":true},"execution_count":null,"outputs":[]}]}