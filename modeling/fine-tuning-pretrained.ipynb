{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9162188,"sourceType":"datasetVersion","datasetId":5535463}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Resnet\n- Prepare data for training/validation. Create dataloader\n- Load in resnet model\n- Create architecture for fine tuning including pytorch/tensorflow boilerplate","metadata":{}},{"cell_type":"code","source":"#!pip install -q --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu118","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install -q spotipy","metadata":{"execution":{"iopub.status.busy":"2024-08-15T14:13:25.601417Z","iopub.execute_input":"2024-08-15T14:13:25.602026Z","iopub.status.idle":"2024-08-15T14:13:40.258639Z","shell.execute_reply.started":"2024-08-15T14:13:25.601988Z","shell.execute_reply":"2024-08-15T14:13:40.257455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa\nimport librosa.display\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport os\nfrom dotenv import dotenv_values \nimport pickle as pkl\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\n\nfrom scipy.spatial.distance import euclidean\nfrom sklearn.model_selection import train_test_split\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration","metadata":{"execution":{"iopub.status.busy":"2024-08-18T03:22:37.619267Z","iopub.execute_input":"2024-08-18T03:22:37.619583Z","iopub.status.idle":"2024-08-18T03:22:37.625820Z","shell.execute_reply.started":"2024-08-18T03:22:37.619559Z","shell.execute_reply":"2024-08-18T03:22:37.624891Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Create Dataset Class","metadata":{}},{"cell_type":"code","source":"class SpectrogramDataset(Dataset):\n    def __init__(self, file_paths, transform=False, sr=22050, n_mels=128):\n        self.file_paths = file_paths\n        self.data_index = self._build_index()\n        self.sr = sr\n        self.n_mels = n_mels\n        self.transform = transform\n        \n    def _build_index(self):\n        index = []\n        for file_idx, file_path in enumerate(self.file_paths):\n            with open(file_path, 'rb') as f:\n                data = pkl.load(f)\n                for i in range(len(data)):\n                    index.append((file_idx, i))\n        return index\n\n    def _get_log_mel_spectrogram(self, y):\n        # Convert to mel spectrogram\n        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n        # Convert to log scale\n        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n        log_mel_spectrogram = torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)\n        return log_mel_spectrogram\n\n    def __len__(self):\n        return len(self.data_index)\n\n    def __getitem__(self, idx):\n        file_idx, data_idx = self.data_index[idx]\n        file_path = self.file_paths[file_idx]\n\n        with open(file_path, 'rb') as f:\n            data = pkl.load(f)\n        \n        row = data.iloc[data_idx]\n        anchor = row['processed_audio'][0]  # (y, sr)\n        positive = row['augmented_audio'][0]\n        negative = row['diff_processed_audio'][0]\n\n        # Convert to log mel spectrograms\n        anchor_mel = self._get_log_mel_spectrogram(anchor)\n        positive_mel = self._get_log_mel_spectrogram(positive)\n        negative_mel = self._get_log_mel_spectrogram(negative)\n        \n        # Apply any transformations\n        if self.transform:\n            anchors = self.transform(anchors)\n            positives = self.transform(positives)\n            negatives = self.transform(negatives)\n\n        return anchor_mel, positive_mel, negative_mel\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T03:22:37.627002Z","iopub.execute_input":"2024-08-18T03:22:37.627291Z","iopub.status.idle":"2024-08-18T03:22:37.639893Z","shell.execute_reply.started":"2024-08-18T03:22:37.627268Z","shell.execute_reply":"2024-08-18T03:22:37.638881Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Split Data and Instantiate Dataset Class","metadata":{}},{"cell_type":"code","source":"file_paths = [f'/kaggle/input/augmented-audio-10k/batch_{i}_augmented.pkl' for i in range(1,10,1)]\n\n# Split the files instaed of actual data into training/val\ntrain_files, val_files = train_test_split(file_paths, test_size=0.2, random_state=123)\n\n# Instantiate Dataset Classes\ntrain_dataset = SpectrogramDataset(train_files)\nval_dataset = SpectrogramDataset(val_files)\n\n# Declare dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T03:22:37.641211Z","iopub.execute_input":"2024-08-18T03:22:37.641546Z","iopub.status.idle":"2024-08-18T03:22:44.050629Z","shell.execute_reply.started":"2024-08-18T03:22:37.641492Z","shell.execute_reply":"2024-08-18T03:22:44.049807Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Delcaring the Model\n- Define architecture: default resnet with adjusted first conv layer and final FC layer to set num params\n- Choose loss function, optimizer, device, etc.","metadata":{}},{"cell_type":"code","source":"# Define pretrained resnet from Torch Vision resnet 18\nclass ResNetEmbedding(nn.Module):\n    def __init__(self, embedding_dim=128, dropout_rate=0.8):\n        # get resnet super class\n        super(ResNetEmbedding, self).__init__()\n        self.resnet = models.resnet18(weights='DEFAULT')\n        # Change structure of first layer to take non RGB images, rest of params same as default\n        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.dropout = nn.Dropout(p=dropout_rate)\n        # Set the last fully connected to a set dimension \"embedding_dim\" instead of default 1000\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embedding_dim)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        return F.normalize(x, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:27:13.324414Z","iopub.execute_input":"2024-08-16T16:27:13.324687Z","iopub.status.idle":"2024-08-16T16:27:13.331870Z","shell.execute_reply.started":"2024-08-16T16:27:13.324664Z","shell.execute_reply":"2024-08-16T16:27:13.330984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WhisperEmbedding(nn.Module):\n    def __init__(self, embedding_dim=128, pretrained_model_name=\"openai/whisper-tiny\"):\n        super(WhisperEmbedding, self).__init__()\n        self.whisper = WhisperForConditionalGeneration.from_pretrained(pretrained_model_name)\n        # Add a final fully connected layer to whisper\n        self.fc = nn.Linear(self.whisper.config.hidden_size, embedding_dim)\n\n    def forward(self, x):\n        # Feed into whisper encoder\n        outputs = self.whisper.encoder(x)\n        # Get the mean of the hidden states to create an embedding\n        embedding = outputs.last_hidden_state.mean(dim=1)\n        # Pass through a linear layer to get the final embedding\n        embedding = self.fc(embedding)\n        # Normalize the embedding\n        return torch.nn.functional.normalize(embedding, p=2, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T03:37:44.720011Z","iopub.execute_input":"2024-08-18T03:37:44.720757Z","iopub.status.idle":"2024-08-18T03:37:44.727699Z","shell.execute_reply.started":"2024-08-18T03:37:44.720719Z","shell.execute_reply":"2024-08-18T03:37:44.726848Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Choose model, loss, and optimizer\nmodel = ResNetEmbedding()\n#model = WhisperEmbedding()\ncriterion = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n\n# Freeze all the layers\nfor param in model.resnet.parameters():\n    param.requires_grad = False\n\n# Turn back on last residual block\nfor param in model.resnet.layer4.parameters():\n    param.requires_grad = True\n\n# Turn back on fully connected layer\nfor param in model.resnet.fc.parameters():\n    param.requires_grad = True\n\n# Use a smaller learning rate for fine-tuning\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-4)\n\n#optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n\n# Declare losses/accuracies\ntrain_losses = []\nval_losses = []\nbaseline_losses = []\n\nnum_epochs = 1\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T03:41:52.172609Z","iopub.execute_input":"2024-08-18T03:41:52.173184Z","iopub.status.idle":"2024-08-18T03:41:54.429901Z","shell.execute_reply.started":"2024-08-18T03:41:52.173152Z","shell.execute_reply":"2024-08-18T03:41:54.428920Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6647b7a147ea4a3d938949b5307e8fdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1c72390acc4838af39e3861d15fbf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7427af6fc8e7463683f4aa0dab0c30ba"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"WhisperEmbedding(\n  (whisper): WhisperForConditionalGeneration(\n    (model): WhisperModel(\n      (encoder): WhisperEncoder(\n        (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n        (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n        (embed_positions): Embedding(1500, 384)\n        (layers): ModuleList(\n          (0-3): 4 x WhisperEncoderLayer(\n            (self_attn): WhisperSdpaAttention(\n              (k_proj): Linear(in_features=384, out_features=384, bias=False)\n              (v_proj): Linear(in_features=384, out_features=384, bias=True)\n              (q_proj): Linear(in_features=384, out_features=384, bias=True)\n              (out_proj): Linear(in_features=384, out_features=384, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      )\n      (decoder): WhisperDecoder(\n        (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n        (embed_positions): WhisperPositionalEmbedding(448, 384)\n        (layers): ModuleList(\n          (0-3): 4 x WhisperDecoderLayer(\n            (self_attn): WhisperSdpaAttention(\n              (k_proj): Linear(in_features=384, out_features=384, bias=False)\n              (v_proj): Linear(in_features=384, out_features=384, bias=True)\n              (q_proj): Linear(in_features=384, out_features=384, bias=True)\n              (out_proj): Linear(in_features=384, out_features=384, bias=True)\n            )\n            (activation_fn): GELUActivation()\n            (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): WhisperSdpaAttention(\n              (k_proj): Linear(in_features=384, out_features=384, bias=False)\n              (v_proj): Linear(in_features=384, out_features=384, bias=True)\n              (q_proj): Linear(in_features=384, out_features=384, bias=True)\n              (out_proj): Linear(in_features=384, out_features=384, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n  )\n  (fc): Linear(in_features=384, out_features=128, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Training Model","metadata":{}},{"cell_type":"code","source":"# Loop over epochs\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    running_train_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Training {epoch+1}/{num_epochs}\", unit=\"batch\")\n\n    # Loop over batches using dataloaders\n    for anchors, positives, negatives in train_loader:\n        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n        optimizer.zero_grad()\n        \n        anchor_embeddings = model(anchors)\n        positive_embeddings = model(positives)\n        negative_embeddings = model(negatives)\n        \n        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n        loss.backward()\n        optimizer.step()\n        \n        running_train_loss += loss.item() * anchors.size(0)\n        # Update the progress bar by the current batch size\n        pbar.update(1)  # Increment the progress bar\n        #pbar.update(anchors.size(0))\n        #pbar.set_postfix(loss=loss.item())\n          \n    train_loss = running_train_loss / len(train_loader.dataset)\n    train_losses.append(train_loss)\n\n    # Turn on validation/eval mode\n    model.eval()\n    running_val_loss = 0.0 \n    baseline_loss = 0.0   \n    val_pbar = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\")\n    # Turn off gradient updates since we're in validation\n    with torch.no_grad():\n        # Batch loop \n        for anchors, positives, negatives in tqdm(val_loader):\n            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n            \n            anchor_embeddings = model(anchors)\n            positive_embeddings = model(positives)\n            negative_embeddings = model(negatives)\n            \n            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n            #val_loss += loss.item()\n\n            running_val_loss += loss.item() * anchors.size(0)\n            # baseline loss\n            baseline_loss += criterion(anchors, positives, negatives).item()\n            \n            # Update the validation progress bar\n            val_pbar.update(1)  # Increment the progress bar\n            #val_pbar.update(anchors.size(0))\n            #val_pbar.set_postfix(loss=loss.item())\n    \n    # Calculate average validation loss over the entire dataset\n    val_loss = running_val_loss / len(val_loader.dataset)\n    val_losses.append(val_loss)\n    # Do the same for the baseline\n    baseline_avg_loss = baseline_loss / len(val_loader.dataset)\n    baseline_losses.append(baseline_avg_loss)\n\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Baseline Loss: {baseline_avg_loss:.4f}\")\n    \n    with open('training_logs.pkl', 'wb') as f:\n        pkl.dump((train_losses, val_losses), f)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T03:42:00.419533Z","iopub.execute_input":"2024-08-18T03:42:00.420354Z","iopub.status.idle":"2024-08-18T03:44:10.330764Z","shell.execute_reply.started":"2024-08-18T03:42:00.420322Z","shell.execute_reply":"2024-08-18T03:44:10.329488Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"Training 1/1:   0%|          | 0/219 [00:00<?, ?batch/s]","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m anchors, positives, negatives \u001b[38;5;241m=\u001b[39m anchors\u001b[38;5;241m.\u001b[39mto(device), positives\u001b[38;5;241m.\u001b[39mto(device), negatives\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m anchor_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m positive_embeddings \u001b[38;5;241m=\u001b[39m model(positives)\n\u001b[1;32m     15\u001b[0m negative_embeddings \u001b[38;5;241m=\u001b[39m model(negatives)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mWhisperEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Feed into whisper encoder\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m(x)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Get the mean of the hidden states to create an embedding\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'WhisperForConditionalGeneration' object has no attribute 'encoder'"],"ename":"AttributeError","evalue":"'WhisperForConditionalGeneration' object has no attribute 'encoder'","output_type":"error"}]},{"cell_type":"code","source":"# Plot loss curves for training\nepochs = range(1, num_epochs + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.plot(epochs, baseline_losses, label='Baseline Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.tight_layout()\nplt.savefig('resnet-loss-plot.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.439153Z","iopub.status.idle":"2024-08-16T16:22:41.439602Z","shell.execute_reply.started":"2024-08-16T16:22:41.439403Z","shell.execute_reply":"2024-08-16T16:22:41.439427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save just the model weights (recommended apparently for portability/compatibility)\ntorch.save(model.state_dict(), 'resnet18_model_weights.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.441097Z","iopub.status.idle":"2024-08-16T16:22:41.441510Z","shell.execute_reply.started":"2024-08-16T16:22:41.441304Z","shell.execute_reply":"2024-08-16T16:22:41.441321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the entire model so we can use it for deployment\ntorch.save(model, 'resnet18_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T16:22:41.443645Z","iopub.status.idle":"2024-08-16T16:22:41.444030Z","shell.execute_reply.started":"2024-08-16T16:22:41.443852Z","shell.execute_reply":"2024-08-16T16:22:41.443869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Deploy on any 2 songs\n- Use model to calculate embeddings (using eval mode specifically and with no gradient updating)\n- NOTE: Right now, we have to have the input data in the correct format: a spectrogram/chromagram/tempogram (generically called \"gram\"). So for any deployment, we'll have to do preprocessing in the streamlit app for example. OR we can have a set of say 10-15 sample songs you can compare where we've already done all of the calculations.\n- **Similarity values key:**\n    - 0.5 to 1: Very similar. Perhaps the same song.\n    - 0 to 0.5: Somewhat similar. Share some key characteristics\n    - -1 to 0: Low to no similarity. Different songs.\n","metadata":{}},{"cell_type":"code","source":"# How to load the model later using just the state dictionary\nmodel = ResNetEmbedding()  # Make sure this matches the architecture you used\nmodel.load_state_dict(torch.load('resnet18_model_weights.pth'))\n\n# If using a GPU\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_embedding(model, audio_data_clip, sr=22050, use_model=True):\n    y = audio_data_clip\n    #y, sr = librosa.load(audio_data_clip, sr=sr)\n    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n    \n    # Convert to tensor and move to the appropriate device\n    mel_tensor = torch.tensor(mel_spectrogram_db, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n    \n    if use_model:\n        # Get the embedding from the model\n        with torch.no_grad():\n            embedding = model(mel_tensor)\n        \n        # Normalize the embedding\n        #embedding = F.normalize(embedding, p=2, dim=1)\n        return embedding\n    else:\n        return mel_tensor\n\n\ndef compute_cosine_similarity(embedding1, embedding2):\n    # Compute cosine similarity\n    cosine_sim = F.cosine_similarity(embedding1, embedding2)\n    return cosine_sim.item()  # Convert to a Python float\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#explore_df = pd.read_pickle('/kaggle/input/augmented-audio-10k/batch_1_augmented.pkl')\n#explore_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test on some random training/validation data as sanity check\n#y1, y2 = explore_df['processed_audio'][10000][0], explore_df['diff_processed_audio'][10000][0]\n#mel1, mel2 = extract_embedding(model, y1), extract_embedding(model, y2)\n#compute_cosine_similarity(mel1, mel2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test on some random training/validation data as sanity check\n#y1, y3 = explore_df['processed_audio'][10000][0], explore_df['augmented_audio'][10000][0]\n#mel1, mel3 = extract_embedding(model, y1), extract_embedding(model, y3)\n#compute_cosine_similarity(mel1, mel3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#criterion(mel1, mel2, mel3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}